{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#This Pipeline sets up batch inference for DialoGPT-large and BlenderBot 1.0\n",
        "\n",
        "Installing huggingface transformers for blenderbot & dialoGPT \\\n",
        "Installing sentence transformers for sentence similarity scores"
      ],
      "metadata": {
        "id": "YRmjKt0-kMsV"
      },
      "id": "YRmjKt0-kMsV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abc9c3c7",
      "metadata": {
        "id": "abc9c3c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99d34c70-e130-428e-fd29-c02b32110753"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.2.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.96)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.11.1+cu111)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.17.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.63.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.4.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.10.0+cu111)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.10.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.11.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.11.6)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.49)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install transformers\n",
        "!pip3 install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d39bdb03",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d39bdb03",
        "outputId": "b1a4a058-5d85-43fc-e127-f3fd58ee0665"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import csv\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
        "# device = 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive and go to SALT Lab folder"
      ],
      "metadata": {
        "id": "cEQYRo4xveKk"
      },
      "id": "cEQYRo4xveKk"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_z5DwtvpYngQ",
        "outputId": "ca7c84ba-5052-4473-a138-ce57b339fd20"
      },
      "id": "_z5DwtvpYngQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "803061ee",
      "metadata": {
        "id": "803061ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a0fa428-fa0c-4f62-8bf4-7db0cc7e851f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/SALT Lab\n"
          ]
        }
      ],
      "source": [
        "path = 'drive/MyDrive/Colab Notebooks/SALT Lab' \n",
        "%cd $path"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load CSVs as pandas dataframes\n",
        "Standard_contexts = unedited contexts \\\n",
        "perturbed_contexts = AAVE contexts \\\n",
        "short_contexts = shorter list of contexts \\"
      ],
      "metadata": {
        "id": "f8kvUiHQvk88"
      },
      "id": "f8kvUiHQvk88"
    },
    {
      "cell_type": "code",
      "source": [
        "standard_path = 'data/reddit_filtered.csv'\n",
        "perturbed_path = 'data/reddit_filtered.csv'\n",
        "standard_df = pd.read_csv(standard_path, index_col = 0)\n",
        "perturbed_df = pd.read_csv(perturbed_path, index_col = 0)"
      ],
      "metadata": {
        "id": "9mEYGZoxbw-0"
      },
      "id": "9mEYGZoxbw-0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "standard_contexts = standard_df['context']\n",
        "perturbed_contexts = perturbed_df['context']\n",
        "short_contexts = standard_df['context'][0:10]"
      ],
      "metadata": {
        "id": "jCrfLOR0cKq8"
      },
      "id": "jCrfLOR0cKq8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set up DialoGPT-large model using huggingface\n",
        "##function get_dialogpt_outputs()\n",
        "@param num_samples: number of samples to input \\\n",
        "@param num_copies: number of outputs per sample \\\n",
        "@param temp: temperature \\\n",
        "@output all outputs (array of size num_samples x num_copies)"
      ],
      "metadata": {
        "id": "QO48-O1tw99W"
      },
      "id": "QO48-O1tw99W"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49aa7737",
      "metadata": {
        "id": "49aa7737"
      },
      "outputs": [],
      "source": [
        "Dialogpt_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
        "Dialogpt_tokenizer.add_special_tokens({'pad_token': '<|endoftext|>'})\n",
        "Dialogpt_model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\").to(device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fb50973",
      "metadata": {
        "id": "8fb50973"
      },
      "outputs": [],
      "source": [
        "def get_dialogpt_outputs(num_samples, num_copies, temp):\n",
        "  # Test out how long the function takes\n",
        "  # start_time = time.time()\n",
        "\n",
        "  batch_size = 5\n",
        "  all_outputs = []\n",
        "  for step in tqdm(range(batch_size, num_samples + 1, batch_size)):\n",
        "      outputs = []\n",
        "      # pull the batched inputs + append EOS token to the end of each input\n",
        "      inputs = list(standard_contexts[step - batch_size:step])\n",
        "      inputs = [i + Dialogpt_tokenizer.eos_token for i in inputs]\n",
        "\n",
        "      # encode the inputs\n",
        "      input_info = Dialogpt_tokenizer(inputs, padding = True, return_tensors = 'pt').to(device=device)\n",
        "      input_ids = input_info['input_ids']\n",
        "      attention_mask = input_info['attention_mask']\n",
        "\n",
        "      # generated a num_samples * num_copies responses\n",
        "      chat_history_ids = Dialogpt_model.generate(\n",
        "          input_ids, \n",
        "          max_length=1000, \n",
        "          do_sample = True,\n",
        "          # top_k=50, \n",
        "          # top_p=0.95,\n",
        "          temperature = temp,\n",
        "          num_return_sequences = num_copies,\n",
        "          pad_token_id = Dialogpt_tokenizer.eos_token_id,\n",
        "          attention_mask = attention_mask)\n",
        "      # output = Dialogpt_tokenizer.decode(chat_history_ids[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "      for i in chat_history_ids:\n",
        "        output = Dialogpt_tokenizer.decode(i[input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "        # print(output)\n",
        "        outputs.append(output)\n",
        "        # pretty print last ouput tokens from bot\n",
        "        # print(\"Input: \" + str(standard_contexts[step]))\n",
        "        # print(\"DialoGPT: {}\".format(Dialogpt_tokenizer.decode(chat_history_ids[:, input_ids.shape[-1]:][0], skip_special_tokens=True)))\n",
        "      outputs = [outputs[x : x+num_copies] for x in range(0, len(outputs), num_copies)]\n",
        "      all_outputs.extend(outputs)\n",
        "      # print(\"Took \" + str(time.time() - start_time) + \" seconds to run\")\n",
        "  return all_outputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##DialoGPT sanity test"
      ],
      "metadata": {
        "id": "ZbwRINi9t6mz"
      },
      "id": "ZbwRINi9t6mz"
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_dialogpt_outputs(5, 5, 0.5))"
      ],
      "metadata": {
        "id": "xcWs5JFdrIM9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da71a372-4514-4bb9-9c7c-8fadba9ffd62"
      },
      "id": "xcWs5JFdrIM9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:05<00:00,  5.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[\"I don't know. I have social anxiety and I'm not American.\", \"It's just a thing that's been around for a long time.\", 'Social anxiety is a symptom of depression', \"Yes, it's an American thing.\", \"It's a social thing.\"], [\"No, he's related to Mr. Riddle\", 'No, just a very good artist.', 'No, but his mother is.', \"No, but that's a great name for a band.\", 'Nope, just a random name.'], ['Hue hue!', 'Gracias!!', 'Gracias!', 'Aqui no te preocupes?', 'Gracias!'], ['I wonder if he was a teacher at the same school as the guy who signed the letter.', 'I know. I was surprised that they signed a letter.', 'I thought he was a teacher?', 'I wonder how much that teacher got for that...', 'Khabib is a fighter, not a teacher.'], [\"I think you're the one with the most comments this week.\", \"I'm a fan of your comment karma\", 'I was the first to comment on this.', 'I thought I was the only one.', \"No you're the one who posted the picture.\"]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set up Blenderbot-1B-distill using huggingface\n",
        "##function get_blenderbot_outputs()\n",
        "@param num_samples: number of samples to input \\\n",
        "@param num_copies: number of outputs per sample \\\n",
        "@param temp: temperature \\\n",
        "@output all outputs (array of size num_samples x num_copies)"
      ],
      "metadata": {
        "id": "bJL-pOb-x86I"
      },
      "id": "bJL-pOb-x86I"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration\n",
        "\n",
        "blenderbot_path = \"facebook/blenderbot-1B-distill\"\n",
        "blenderbot_tokenizer = BlenderbotTokenizer.from_pretrained(blenderbot_path)\n",
        "blenderbot_model = BlenderbotForConditionalGeneration.from_pretrained(blenderbot_path).to(device=device)"
      ],
      "metadata": {
        "id": "FvAn90F1e6mE"
      },
      "id": "FvAn90F1e6mE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's chat for 5 lines\n",
        "import time\n",
        "def get_blenderbot_outputs(num_samples, num_copies, temp):\n",
        "  start_time = time.time()\n",
        "  all_outputs = []\n",
        "  batch_size = 2\n",
        "  for step in tqdm(range(batch_size, num_samples + 1, batch_size)):\n",
        "    outputs = []\n",
        "    # Pull batch_size number of inputs and preprocess them\n",
        "    samples = list(standard_contexts[step - batch_size : step])\n",
        "    input_info = blenderbot_tokenizer(samples, padding = True, return_tensors='pt').to(device=device)\n",
        "    input_ids = input_info['input_ids']\n",
        "    attention_mask = input_info['attention_mask']\n",
        "\n",
        "    # generated batch_size x num_copies responses\n",
        "    chat_history_ids = blenderbot_model.generate(\n",
        "        input_ids, \n",
        "        max_length=1000, \n",
        "        do_sample = True,\n",
        "        # top_k = 50, \n",
        "        # top_p = 0.95,\n",
        "        temperature = temp,\n",
        "        num_return_sequences = num_copies,\n",
        "        attention_mask = attention_mask)\n",
        "    \n",
        "    # Decode outputs\n",
        "    for i in chat_history_ids:\n",
        "      output = blenderbot_tokenizer.decode(i, skip_special_tokens=True)\n",
        "      outputs.append(output)\n",
        "    # Separate outputs into batch_size x num_copies (they come in one big array)\n",
        "    outputs = [outputs[x : x + num_copies] for x in range(0, len(outputs), num_copies)]\n",
        "    all_outputs.extend(outputs)\n",
        "\n",
        "  # print(\"my program took \" + str(time.time() - start_time) + \" seconds to run\")\n",
        "  return all_outputs"
      ],
      "metadata": {
        "id": "LtyVTDr3pAYU"
      },
      "id": "LtyVTDr3pAYU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Blenderbot sanity test"
      ],
      "metadata": {
        "id": "82wf7dITu-S_"
      },
      "id": "82wf7dITu-S_"
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_blenderbot_outputs(2, 10, 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIiZN59cyUDJ",
        "outputId": "25683f24-4ace-4e65-d11c-62fff1cea48c"
      },
      "id": "rIiZN59cyUDJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:14<00:00, 14.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[\" I'm not sure, but I do know that social anxiety is a fear of social situations.\", \" I'm not sure, but I think it has something to do with the fact that a lot of people are introverted.\", \" I'm not really sure, but I do know that it is a mental disorder where people feel anxious in social situations.\", \" I'm not sure, but I do know that social anxiety is a fear of social interaction.\", \" I'm not sure, but I do know that social anxiety is a fear of social situations.\", \" I'm not sure, but I do know that social anxiety is a mental disorder characterized by fear of social situations.\", \" I'm not sure, but I do know it can be caused by a combination of genetic and environmental factors.\", \" I'm not sure, but I do know that social anxiety disorder is a mental disorder characterized by excessive fear of social situations.\", \" I'm not sure, but I do know that social anxiety is a fear of social situations.\", \" I'm not sure, but I do know that social anxiety disorder is a mental disorder.\"], [\" No, he's not.  I don't know who that is.  Why do you ask?\", \" I'm not sure, but I do know that he was born and raised in Houston, Texas.\", \" No, he's not.  But he does look a lot like him.  I don't know if he's related to him or not.\", \" No, he is not. I don't know what I would have done if he was.\", ' No, he is not.  He was born and raised in Los Angeles, California.', \" I don't think so, but I do know that he was born in 1948.\", \" I'm not sure who that is, but I do know that he was born in 1964.\", \" No, he's not.  I don't even know who that is.  Why do you ask?\", \" No, he's not.  He's a musician.  I love his music.\", \" I don't think so, but I do know that he was born in 1948.\"]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dialog model temperature volatility tests\n",
        "In this section we will test how diverse model outputs are at different temperature values. \\\n",
        "We will use BLEU and sentence embedding similarites to gauge model output. \\ "
      ],
      "metadata": {
        "id": "4g-r5DaczUlY"
      },
      "id": "4g-r5DaczUlY"
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "import nltk\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "semantic_similarity_model = SentenceTransformer('all-mpnet-base-v2')"
      ],
      "metadata": {
        "id": "_Iev06_egnDB"
      },
      "id": "_Iev06_egnDB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##BLEU and Semantic Similarity Sanity Tests (Just make sure they're working)"
      ],
      "metadata": {
        "id": "TSwOLM3gu0_5"
      },
      "id": "TSwOLM3gu0_5"
    },
    {
      "cell_type": "code",
      "source": [
        "def bleu_sanity_test():\n",
        "  reference = \"I think I might have to go with you.\"\n",
        "  hypothesis = \"I think I might have to go with poo.\"\n",
        "  score = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)\n",
        "  return score\n",
        "def semantic_similarity_sanity_test():\n",
        "  reference = \"I think I might have to go with you.\"\n",
        "  hypothesis = \"I think I might have to go with poo.\"\n",
        "  sentence_embeddings = semantic_similarity_model.encode([reference, hypothesis])\n",
        "  score = float(util.pytorch_cos_sim(sentence_embeddings[0], sentence_embeddings[1]))\n",
        "  return score\n",
        "\n",
        "print(bleu_sanity_test())\n",
        "print(semantic_similarity_sanity_test())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeKRwaJvfr3W",
        "outputId": "98e36bbe-d900-448e-f8eb-e0473159172f"
      },
      "id": "TeKRwaJvfr3W",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.897426966486071\n",
            "0.32228589057922363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ss_tests():\n",
        "  test = [\"I'm not sure what you're trying to say.\", \"You're a good man.\", \"I'll be your new best friend.\", 'You have a lot of comments.', \"It's a bot.\", 'I know that feel bro.', 'You were the one that said it, though.', \"This is why I don't post on this sub anymore.\", \"I'm so happy you're back! We all miss you.\", 'I got you fam.']  \n",
        "  for i in range(len(test) - 1):\n",
        "    print(test[i])\n",
        "    print(test[i + 1])\n",
        "    reference = test[i]\n",
        "    hypothesis = test[i + 1]\n",
        "    bleu_score = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)\n",
        "    sentence_embeddings = semantic_similarity_model.encode([reference, hypothesis])\n",
        "    ss_score = float(util.pytorch_cos_sim(sentence_embeddings[0], sentence_embeddings[1]))\n",
        "    print(\"bleu score = \" + str(bleu_score))\n",
        "    print(\"ss_score = \" + str(ss_score))\n",
        "ss_tests()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRKMgeVIXPZF",
        "outputId": "bd3a1693-356a-4af2-e27b-3a877149f517"
      },
      "id": "RRKMgeVIXPZF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm not sure what you're trying to say.\n",
            "You're a good man.\n",
            "bleu score = 0.10529499920771897\n",
            "ss_score = 0.08433771133422852\n",
            "You're a good man.\n",
            "I'll be your new best friend.\n",
            "bleu score = 0.4057108879449503\n",
            "ss_score = 0.20012082159519196\n",
            "I'll be your new best friend.\n",
            "You have a lot of comments.\n",
            "bleu score = 0.5020923136190463\n",
            "ss_score = 0.19615882635116577\n",
            "You have a lot of comments.\n",
            "It's a bot.\n",
            "bleu score = 0.09213888389040659\n",
            "ss_score = 0.17537584900856018\n",
            "It's a bot.\n",
            "I know that feel bro.\n",
            "bleu score = 0.38260294162784475\n",
            "ss_score = 0.10398069024085999\n",
            "I know that feel bro.\n",
            "You were the one that said it, though.\n",
            "bleu score = 0.15257340614701648\n",
            "ss_score = 0.13090452551841736\n",
            "You were the one that said it, though.\n",
            "This is why I don't post on this sub anymore.\n",
            "bleu score = 0.27486893254130634\n",
            "ss_score = 0.1572614163160324\n",
            "This is why I don't post on this sub anymore.\n",
            "I'm so happy you're back! We all miss you.\n",
            "bleu score = 0.5203529238931589\n",
            "ss_score = 0.12132645398378372\n",
            "I'm so happy you're back! We all miss you.\n",
            "I got you fam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bleu score = 0.030983872032022023\n",
            "ss_score = 0.15337564051151276\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bleu & semantic similarity functions on lists of num_samples x num_copies\n",
        "function bleu() \\\n",
        "@return the average pairwise BLEU score \\\n",
        "\n",
        "function semantic_similarity() \\\n",
        "@return the average pairwise semantic similarity (based on sentence embeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "9lbary1h0Xvq"
      },
      "id": "9lbary1h0Xvq"
    },
    {
      "cell_type": "code",
      "source": [
        "def list_to_json(data, name):\n",
        "  with open(name, \"w\") as f:\n",
        "    json.dump(data, f)\n",
        "  print(\"saved as json\")"
      ],
      "metadata": {
        "id": "jdgdWgZ616Df"
      },
      "id": "jdgdWgZ616Df",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bleu(all_outputs):\n",
        "  scores = []\n",
        "  for i in all_outputs:\n",
        "    pairs = list(combinations(i, 2))\n",
        "    for i in pairs:\n",
        "      ref, trans = i\n",
        "      score = nltk.translate.bleu_score.sentence_bleu([ref], trans)\n",
        "      scores.append(score)\n",
        "  return scores, sum(scores) / len(scores)\n",
        "\n",
        "def semantic_similarity(all_outputs):\n",
        "  scores = []\n",
        "\n",
        "  for i in all_outputs:\n",
        "    sentence_embeddings = semantic_similarity_model.encode(i)\n",
        "    pairs = list(combinations(sentence_embeddings, 2))\n",
        "    for i in pairs:\n",
        "      a, b = i\n",
        "      score = float(util.pytorch_cos_sim(a, b))\n",
        "      scores.append(score)\n",
        "  return scores, sum(scores) / len(scores)\n",
        "def pairwise_bleu(first, second):\n",
        "  avg_scores = []\n",
        "  for i in first:\n",
        "    scores = []\n",
        "    for j in second:\n",
        "      score = nltk.translate.bleu_score.sentence_bleu([i], j)\n",
        "      scores.append(score)\n",
        "    avg_scores.append(sum(scores) / len(scores))\n",
        "  return sum(avg_scores) / len(avg_scores)\n",
        "def pairwise_semantic_similarity(first, second):\n",
        "  avg_scores = []\n",
        "  first_sentence_embeddings = semantic_similarity_model.encode(first)\n",
        "  second_sentence_embeddings = semantic_similarity_model.encode(second)\n",
        "  for i in first_sentence_embeddings:\n",
        "    scores = []\n",
        "    for j in second_sentence_embeddings:\n",
        "      score = float(util.pytorch_cos_sim(i, j))\n",
        "      scores.append(score)\n",
        "    avg_scores.append(sum(scores) / len(scores))\n",
        "  return sum(avg_scores) / len(avg_scores)\n",
        "# all_outputs = [[\"hi how are you doing today\", \"hello how is your mother\", \"you looking good today ;)\"]]"
      ],
      "metadata": {
        "id": "f47MqfXLjIx2"
      },
      "id": "f47MqfXLjIx2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model_volatility gets all SS/BLEU similarity scores from DialoGPT or Blenderbot."
      ],
      "metadata": {
        "id": "1vjO38voG9y9"
      },
      "id": "1vjO38voG9y9"
    },
    {
      "cell_type": "code",
      "source": [
        "def model_volatility(num_samples, num_copies, max_temp):\n",
        "  dialogpt_bleu_scores = []\n",
        "  dialogpt_bleu_avgs = []\n",
        "\n",
        "  dialogpt_ss_scores = []\n",
        "  dialogpt_ss_avgs = []\n",
        "\n",
        "  blenderbot_bleu_scores = []\n",
        "  blenderbot_bleu_avgs = []\n",
        "\n",
        "  blenderbot_ss_scores = []\n",
        "  blenderbot_ss_avgs = []\n",
        "  # for temp in range(2, int(7 * max_temp)):\n",
        "  #   temp = temp / 10\n",
        "  #   dialogpt_outputs = get_dialogpt_outputs(num_samples, num_copies, temp)\n",
        "\n",
        "  #   dialogpt_bleu_score, dialogpt_bleu_avg = bleu(dialogpt_outputs)\n",
        "  #   dialogpt_bleu_scores.append(dialogpt_bleu_score)\n",
        "  #   dialogpt_bleu_avgs.append(dialogpt_bleu_avg)\n",
        "\n",
        "  #   dialogpt_ss_score, dialogpt_avg = semantic_similarity(dialogpt_outputs)\n",
        "  #   dialogpt_ss_scores.append(dialogpt_ss_score)\n",
        "  #   dialogpt_ss_avgs.append(dialogpt_avg)\n",
        "\n",
        "  # list_to_json(dialogpt_bleu_scores, \"dialogpt_bleu_scores.json\")\n",
        "  # list_to_json(dialogpt_bleu_avgs, \"dialogpt_bleu_avgs.json\")\n",
        "\n",
        "  # list_to_json(dialogpt_ss_scores, \"dialogpt_ss_scores.json\")\n",
        "  # list_to_json(dialogpt_ss_avgs, \"dialogpt_ss_avgs.json\")\n",
        "\n",
        "  for temp in range(8, int(10 * max_temp)):\n",
        "\n",
        "    temp = temp / 10\n",
        "    blenderbot_outputs = get_blenderbot_outputs(num_samples, num_copies * 2, temp)\n",
        "\n",
        "    blenderbot_bleu_score, blenderbot_bleu_avg = bleu(blenderbot_outputs)\n",
        "    blenderbot_bleu_scores.append(blenderbot_bleu_score)\n",
        "    blenderbot_bleu_avgs.append(blenderbot_bleu_avg)\n",
        "\n",
        "    blenderbot_ss_score, blenderbot_ss_avg = semantic_similarity(blenderbot_outputs)\n",
        "    blenderbot_ss_scores.append(blenderbot_ss_score)\n",
        "    blenderbot_ss_avgs.append(blenderbot_ss_avg)\n",
        "\n",
        "  list_to_json(blenderbot_bleu_scores, \"blenderbot_bleu_scores.json\")\n",
        "  list_to_json(blenderbot_bleu_avgs, \"blenderbot_bleu_avgs.json\")\n",
        "\n",
        "  list_to_json(blenderbot_ss_scores, \"blenderbot_ss_scores.json\")\n",
        "  list_to_json(blenderbot_ss_avgs, \"blenderbot_ss_avgs.json\")\n",
        "\n",
        "model_volatility(60, 10, 1.7)"
      ],
      "metadata": {
        "id": "Mz5ynhNL7t7J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "40f3be10-208a-46d7-c32b-b8b88adb6c6d"
      },
      "id": "Mz5ynhNL7t7J",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-f984a55740a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0mlist_to_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblenderbot_ss_avgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"blenderbot_ss_avgs.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mmodel_volatility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-f984a55740a6>\u001b[0m in \u001b[0;36mmodel_volatility\u001b[0;34m(num_samples, num_copies, max_temp)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mblenderbot_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_blenderbot_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_copies\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mblenderbot_bleu_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblenderbot_bleu_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblenderbot_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_blenderbot_outputs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pairwise_model_volatility gets normalized scores (average of all pairwise scores on 5 inputs of A and 5 inputs of B) on DialoGPT and Blenderbot"
      ],
      "metadata": {
        "id": "RaSSdYvTHI4K"
      },
      "id": "RaSSdYvTHI4K"
    },
    {
      "cell_type": "code",
      "source": [
        "def pairwise_model_volatility(num_samples, num_copies, max_temp):\n",
        "  dialogpt_bleu_avgs = []\n",
        "  dialogpt_ss_avgs = []\n",
        "  dialogpt_all_outputs = []\n",
        "  for temp in range(3, max_temp):\n",
        "    dialogpt_temp_bleu_avgs = []\n",
        "    dialogpt_temp_ss_avgs = []\n",
        "\n",
        "    temp = temp / 10\n",
        "    dialogpt_outputs = get_dialogpt_outputs(num_samples, num_copies * 2, temp)\n",
        "    dialogpt_all_outputs.append(dialogpt_outputs)\n",
        "    for i in dialogpt_outputs:\n",
        "      first = i[0:num_copies]\n",
        "      second = i[num_copies:]\n",
        "      dialogpt_bleu_avg = pairwise_bleu(first, second)\n",
        "      dialogpt_temp_bleu_avgs.append(dialogpt_bleu_avg)\n",
        "\n",
        "      dialogpt_ss_avg = pairwise_semantic_similarity(first, second)\n",
        "      dialogpt_temp_ss_avgs.append(dialogpt_ss_avg)\n",
        "\n",
        "    dialogpt_bleu_avgs.append(dialogpt_temp_bleu_avgs)\n",
        "    dialogpt_ss_avgs.append(dialogpt_temp_ss_avgs)\n",
        "\n",
        "  list_to_json(dialogpt_bleu_avgs, \"data/dialogpt_bleu_pairwise_avgs.json\")\n",
        "  list_to_json(dialogpt_ss_avgs, \"data/dialogpt_ss_pairwise_avgs.json\")\n",
        "  list_to_json(dialogpt_all_outputs, \"data/dialogpt_pairwise_outputs.json\")\n",
        "  # blenderbot_bleu_avgs = []\n",
        "  # blenderbot_ss_avgs = []\n",
        "\n",
        "  # for temp in range(11, max_temp):\n",
        "  #   blenderbot_temp_bleu_avgs = []\n",
        "  #   blenderbot_temp_ss_avgs = []\n",
        "\n",
        "  #   temp = temp / 10\n",
        "  #   blenderbot_outputs = get_blenderbot_outputs(num_samples, num_copies * 2, temp)\n",
        "\n",
        "  #   for output in blenderbot_outputs:\n",
        "  #     first = output[0:num_copies]\n",
        "  #     second = output[num_copies:]\n",
        "  #     blenderbot_bleu_avg = pairwise_bleu(first, second)\n",
        "  #     blenderbot_temp_bleu_avgs.append(blenderbot_bleu_avg)\n",
        "\n",
        "  #     blenderbot_ss_avg = pairwise_semantic_similarity(first, second)\n",
        "  #     blenderbot_temp_ss_avgs.append(blenderbot_ss_avg)\n",
        "\n",
        "  #   blenderbot_bleu_avgs.append(blenderbot_temp_bleu_avgs)\n",
        "  #   blenderbot_ss_avgs.append(blenderbot_temp_ss_avgs)\n",
        "\n",
        "  # list_to_json(blenderbot_bleu_avgs, \"data/blenderbot_bleu_pairwise_avgs.json\")\n",
        "  # list_to_json(blenderbot_ss_avgs, \"data/blenderbot_ss_pairwise_avgs.json\")\n",
        "\n",
        "pairwise_model_volatility(1000, 5, 6)"
      ],
      "metadata": {
        "id": "jxdksxb-BjDL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "933002e5-bf9e-42bd-9aaa-fa4a396c57eb"
      },
      "id": "jxdksxb-BjDL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [11:16<00:00,  3.38s/it]\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "100%|██████████| 200/200 [10:25<00:00,  3.13s/it]\n",
            "100%|██████████| 200/200 [10:33<00:00,  3.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saved as json\n",
            "saved as json\n",
            "saved as json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def data_viz(scores, start_temp):\n",
        "  y = scores\n",
        "  x = (np.arange(len(y)) / 10) + start_temp\n",
        "  plt.plot(x, y)\n",
        "  plt.ylabel(\"# of subreddits\")\n",
        "  plt.xlabel(\"subreddit size\")\n",
        "  plt.show()\n",
        "  "
      ],
      "metadata": {
        "id": "44j31vHGMhpO"
      },
      "id": "44j31vHGMhpO",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Double_Pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}