{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#This Pipeline sets up batch inference for DialoGPT-large and BlenderBot 1.0\n",
        "\n",
        "Installing huggingface transformers for blenderbot & dialoGPT \\\n",
        "Installing sentence transformers for sentence similarity scores"
      ],
      "metadata": {
        "id": "YRmjKt0-kMsV"
      },
      "id": "YRmjKt0-kMsV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abc9c3c7",
      "metadata": {
        "id": "abc9c3c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13f03698-bc29-4081-dc26-a4fa614f94d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.2.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.63.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.96)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.4.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.10.0+cu111)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.11.1+cu111)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.17.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.10.0.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.47)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.11.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.11.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install transformers\n",
        "!pip3 install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d39bdb03",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d39bdb03",
        "outputId": "0866533c-4ec6-4a66-e196-7fe39b7ec816"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import csv\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
        "# device = 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive and go to SALT Lab folder"
      ],
      "metadata": {
        "id": "cEQYRo4xveKk"
      },
      "id": "cEQYRo4xveKk"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_z5DwtvpYngQ",
        "outputId": "c734a682-e2e5-4fac-fe42-c0475abc8b18"
      },
      "id": "_z5DwtvpYngQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "803061ee",
      "metadata": {
        "id": "803061ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "540c0cd0-b613-422a-b69b-d8b62005cc16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/SALT Lab\n"
          ]
        }
      ],
      "source": [
        "path = 'drive/MyDrive/Colab Notebooks/SALT Lab' \n",
        "%cd $path"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load CSVs as pandas dataframes\n",
        "Standard_contexts = unedited contexts \\\n",
        "perturbed_contexts = AAVE contexts \\\n",
        "short_contexts = shorter list of contexts \\"
      ],
      "metadata": {
        "id": "f8kvUiHQvk88"
      },
      "id": "f8kvUiHQvk88"
    },
    {
      "cell_type": "code",
      "source": [
        "standard_path = 'data/reddit_filtered.csv'\n",
        "perturbed_path = 'data/reddit_filtered.csv'\n",
        "standard_df = pd.read_csv(standard_path, index_col = 0)\n",
        "perturbed_df = pd.read_csv(perturbed_path, index_col = 0)"
      ],
      "metadata": {
        "id": "9mEYGZoxbw-0"
      },
      "id": "9mEYGZoxbw-0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "standard_contexts = standard_df['context']\n",
        "perturbed_contexts = perturbed_df['context']\n",
        "short_contexts = standard_df['context'][0:10]"
      ],
      "metadata": {
        "id": "jCrfLOR0cKq8"
      },
      "id": "jCrfLOR0cKq8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set up DialoGPT-large model using huggingface\n",
        "##function get_dialogpt_outputs()\n",
        "@param num_samples: number of samples to input \\\n",
        "@param num_copies: number of outputs per sample \\\n",
        "@param temp: temperature \\\n",
        "@output all outputs (array of size num_samples x num_copies)"
      ],
      "metadata": {
        "id": "QO48-O1tw99W"
      },
      "id": "QO48-O1tw99W"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49aa7737",
      "metadata": {
        "id": "49aa7737"
      },
      "outputs": [],
      "source": [
        "Dialogpt_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
        "Dialogpt_tokenizer.add_special_tokens({'pad_token': '<|endoftext|>'})\n",
        "Dialogpt_model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\").to(device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fb50973",
      "metadata": {
        "id": "8fb50973"
      },
      "outputs": [],
      "source": [
        "def get_dialogpt_outputs(num_samples, num_copies, temp):\n",
        "  # Test out how long the function takes\n",
        "  # start_time = time.time()\n",
        "\n",
        "  batch_size = 10\n",
        "  all_outputs = []\n",
        "  for step in tqdm(range(batch_size, num_samples + 1, batch_size)):\n",
        "      outputs = []\n",
        "      # pull the batched inputs + append EOS token to the end of each input\n",
        "      inputs = list(standard_contexts[step - batch_size:step])\n",
        "      inputs = [i + Dialogpt_tokenizer.eos_token for i in inputs]\n",
        "\n",
        "      # encode the inputs\n",
        "      input_info = Dialogpt_tokenizer(inputs, padding = True, return_tensors = 'pt').to(device=device)\n",
        "      input_ids = input_info['input_ids']\n",
        "      attention_mask = input_info['attention_mask']\n",
        "\n",
        "      # generated a num_samples * num_copies responses\n",
        "      chat_history_ids = Dialogpt_model.generate(\n",
        "          input_ids, \n",
        "          max_length=1000, \n",
        "          do_sample = True,\n",
        "          # top_k=50, \n",
        "          # top_p=0.95,\n",
        "          temperature = temp,\n",
        "          num_return_sequences = num_copies,\n",
        "          pad_token_id = Dialogpt_tokenizer.eos_token_id,\n",
        "          attention_mask = attention_mask)\n",
        "      # output = Dialogpt_tokenizer.decode(chat_history_ids[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "      for i in chat_history_ids:\n",
        "        output = Dialogpt_tokenizer.decode(i[input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "        # print(output)\n",
        "        outputs.append(output)\n",
        "        # pretty print last ouput tokens from bot\n",
        "        # print(\"Input: \" + str(standard_contexts[step]))\n",
        "        # print(\"DialoGPT: {}\".format(Dialogpt_tokenizer.decode(chat_history_ids[:, input_ids.shape[-1]:][0], skip_special_tokens=True)))\n",
        "      outputs = [outputs[x : x+num_copies] for x in range(0, len(outputs), num_copies)]\n",
        "      all_outputs.extend(outputs)\n",
        "      # print(\"Took \" + str(time.time() - start_time) + \" seconds to run\")\n",
        "  return all_outputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##DialoGPT sanity test"
      ],
      "metadata": {
        "id": "ZbwRINi9t6mz"
      },
      "id": "ZbwRINi9t6mz"
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_dialogpt_outputs(5, 5, 0.5))"
      ],
      "metadata": {
        "id": "xcWs5JFdrIM9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c755fbe2-3fb6-4530-96a2-2caf83d34c13"
      },
      "id": "xcWs5JFdrIM9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[\"It's a social anxiety thing that's been happening for a while.\", \"I guess it's a thing that happens in the US, and it's a thing that happens in the UK too.\", 'Social anxiety is a very common symptom for many people in the US.', 'I have social anxiety. I feel like I am the only person in the world who has social anxiety.', 'Social anxiety is a social phenomenon. Everyone has it.'], ['I think so', \"No. He's related to Mr. Ditka.\", \"Nope. He's related to the guy who is also related to Mr. M.\", 'No, but I think he is related to the guy who played the guitar for the Dead.', 'He is not related to Mr. Manson.'], ['Gracias!', 'Gracias!', 'Gracias!', 'You are so very welcome!', 'Gracias, mi amor!'], ['That was a really good sign.', 'I wonder if he was a teacher on the show.', 'He was a teacher for his whole life and his whole life he was a teacher.', \"I don't think that's a teacher.\", 'He signed a letter?'], ['I have the highest comment karma of anyone I know.', 'How many comments do you have?', \"You're not the only one.\", \"I don't think anyone can take this guy seriously anymore.\", \"I'm the one with the most comments this week.\"]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set up Blenderbot-1B-distill using huggingface\n",
        "##function get_blenderbot_outputs()\n",
        "@param num_samples: number of samples to input \\\n",
        "@param num_copies: number of outputs per sample \\\n",
        "@param temp: temperature \\\n",
        "@output all outputs (array of size num_samples x num_copies)"
      ],
      "metadata": {
        "id": "bJL-pOb-x86I"
      },
      "id": "bJL-pOb-x86I"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration\n",
        "\n",
        "blenderbot_path = \"facebook/blenderbot-1B-distill\"\n",
        "blenderbot_tokenizer = BlenderbotTokenizer.from_pretrained(blenderbot_path)\n",
        "blenderbot_model = BlenderbotForConditionalGeneration.from_pretrained(blenderbot_path).to(device=device)"
      ],
      "metadata": {
        "id": "FvAn90F1e6mE"
      },
      "id": "FvAn90F1e6mE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's chat for 5 lines\n",
        "import time\n",
        "def get_blenderbot_outputs(num_samples, num_copies, temp):\n",
        "  start_time = time.time()\n",
        "  all_outputs = []\n",
        "  batch_size = 2\n",
        "  for step in tqdm(range(batch_size, num_samples + 1, batch_size)):\n",
        "    outputs = []\n",
        "    # Pull batch_size number of inputs and preprocess them\n",
        "    samples = list(standard_contexts[step - batch_size : step])\n",
        "    input_info = blenderbot_tokenizer(samples, padding = True, return_tensors='pt').to(device=device)\n",
        "    input_ids = input_info['input_ids']\n",
        "    attention_mask = input_info['attention_mask']\n",
        "\n",
        "    # generated batch_size x num_copies responses\n",
        "    chat_history_ids = blenderbot_model.generate(\n",
        "        input_ids, \n",
        "        max_length=1000, \n",
        "        do_sample = True,\n",
        "        # top_k = 50, \n",
        "        # top_p = 0.95,\n",
        "        temperature = temp,\n",
        "        num_return_sequences = num_copies,\n",
        "        attention_mask = attention_mask)\n",
        "    \n",
        "    # Decode outputs\n",
        "    for i in chat_history_ids:\n",
        "      output = blenderbot_tokenizer.decode(i, skip_special_tokens=True)\n",
        "      outputs.append(output)\n",
        "    # Separate outputs into batch_size x num_copies (they come in one big array)\n",
        "    outputs = [outputs[x : x + num_copies] for x in range(0, len(outputs), num_copies)]\n",
        "    all_outputs.extend(outputs)\n",
        "\n",
        "  # print(\"my program took \" + str(time.time() - start_time) + \" seconds to run\")\n",
        "  return all_outputs"
      ],
      "metadata": {
        "id": "LtyVTDr3pAYU"
      },
      "id": "LtyVTDr3pAYU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Blenderbot sanity test"
      ],
      "metadata": {
        "id": "82wf7dITu-S_"
      },
      "id": "82wf7dITu-S_"
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_blenderbot_outputs(2, 10, 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIiZN59cyUDJ",
        "outputId": "25683f24-4ace-4e65-d11c-62fff1cea48c"
      },
      "id": "rIiZN59cyUDJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:14<00:00, 14.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[\" I'm not sure, but I do know that social anxiety is a fear of social situations.\", \" I'm not sure, but I think it has something to do with the fact that a lot of people are introverted.\", \" I'm not really sure, but I do know that it is a mental disorder where people feel anxious in social situations.\", \" I'm not sure, but I do know that social anxiety is a fear of social interaction.\", \" I'm not sure, but I do know that social anxiety is a fear of social situations.\", \" I'm not sure, but I do know that social anxiety is a mental disorder characterized by fear of social situations.\", \" I'm not sure, but I do know it can be caused by a combination of genetic and environmental factors.\", \" I'm not sure, but I do know that social anxiety disorder is a mental disorder characterized by excessive fear of social situations.\", \" I'm not sure, but I do know that social anxiety is a fear of social situations.\", \" I'm not sure, but I do know that social anxiety disorder is a mental disorder.\"], [\" No, he's not.  I don't know who that is.  Why do you ask?\", \" I'm not sure, but I do know that he was born and raised in Houston, Texas.\", \" No, he's not.  But he does look a lot like him.  I don't know if he's related to him or not.\", \" No, he is not. I don't know what I would have done if he was.\", ' No, he is not.  He was born and raised in Los Angeles, California.', \" I don't think so, but I do know that he was born in 1948.\", \" I'm not sure who that is, but I do know that he was born in 1964.\", \" No, he's not.  I don't even know who that is.  Why do you ask?\", \" No, he's not.  He's a musician.  I love his music.\", \" I don't think so, but I do know that he was born in 1948.\"]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dialog model temperature volatility tests\n",
        "In this section we will test how diverse model outputs are at different temperature values. \\\n",
        "We will use BLEU and sentence embedding similarites to gauge model output. \\ "
      ],
      "metadata": {
        "id": "4g-r5DaczUlY"
      },
      "id": "4g-r5DaczUlY"
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "import nltk\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "semantic_similarity_model = SentenceTransformer('all-mpnet-base-v2')"
      ],
      "metadata": {
        "id": "_Iev06_egnDB"
      },
      "id": "_Iev06_egnDB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##BLEU and Semantic Similarity Sanity Tests (Just make sure they're working)"
      ],
      "metadata": {
        "id": "TSwOLM3gu0_5"
      },
      "id": "TSwOLM3gu0_5"
    },
    {
      "cell_type": "code",
      "source": [
        "def bleu_sanity_test():\n",
        "  reference = \"I think I might have to go with you.\"\n",
        "  hypothesis = \"I think I might have to go with poo.\"\n",
        "  score = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)\n",
        "  return score\n",
        "def semantic_similarity_sanity_test():\n",
        "  reference = \"I think I might have to go with you.\"\n",
        "  hypothesis = \"I think I might have to go with poo.\"\n",
        "  sentence_embeddings = semantic_similarity_model.encode([reference, hypothesis])\n",
        "  score = float(util.pytorch_cos_sim(sentence_embeddings[0], sentence_embeddings[1]))\n",
        "  return score\n",
        "\n",
        "print(bleu_sanity_test())\n",
        "print(semantic_similarity_sanity_test())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeKRwaJvfr3W",
        "outputId": "dbad9e6c-f558-401e-f45a-a0b7dd27a40f"
      },
      "id": "TeKRwaJvfr3W",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.897426966486071\n",
            "0.3222857713699341\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bleu & semantic similarity functions on lists of num_samples x num_copies\n",
        "function bleu() \\\n",
        "@return the average pairwise BLEU score \\\n",
        "\n",
        "function semantic_similarity() \\\n",
        "@return the average pairwise semantic similarity (based on sentence embeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "9lbary1h0Xvq"
      },
      "id": "9lbary1h0Xvq"
    },
    {
      "cell_type": "code",
      "source": [
        "def list_to_json(data, name):\n",
        "  with open(name, \"w\") as f:\n",
        "    json.dump(data, f)\n",
        "  print(\"saved as json\")"
      ],
      "metadata": {
        "id": "jdgdWgZ616Df"
      },
      "id": "jdgdWgZ616Df",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bleu(all_outputs):\n",
        "  scores = []\n",
        "  for i in all_outputs:\n",
        "    pairs = list(combinations(i, 2))\n",
        "    for i in pairs:\n",
        "      ref, trans = i\n",
        "      score = nltk.translate.bleu_score.sentence_bleu([ref], trans)\n",
        "      scores.append(score)\n",
        "  return scores, sum(scores) / len(scores)\n",
        "\n",
        "def semantic_similarity(all_outputs):\n",
        "  scores = []\n",
        "\n",
        "  for i in all_outputs:\n",
        "    sentence_embeddings = semantic_similarity_model.encode(i)\n",
        "    pairs = list(combinations(sentence_embeddings, 2))\n",
        "    for i in pairs:\n",
        "      a, b = i\n",
        "      score = float(util.pytorch_cos_sim(a, b))\n",
        "      scores.append(score)\n",
        "  return scores, sum(scores) / len(scores)\n",
        "def pairwise_bleu(first, second):\n",
        "  avg_scores = []\n",
        "  for i in first:\n",
        "    scores = []\n",
        "    for j in second:\n",
        "      score = nltk.translate.bleu_score.sentence_bleu([i], j)\n",
        "      scores.append(score)\n",
        "    avg_scores.append(sum(scores) / len(scores))\n",
        "  return sum(avg_scores) / len(avg_scores)\n",
        "def pairwise_semantic_similarity(first, second):\n",
        "  avg_scores = []\n",
        "  first_sentence_embeddings = semantic_similarity_model.encode(first)\n",
        "  second_sentence_embeddings = semantic_similarity_model.encode(second)\n",
        "  for i in first_sentence_embeddings:\n",
        "    scores = []\n",
        "    for j in second_sentence_embeddings:\n",
        "      score = float(util.pytorch_cos_sim(i, j))\n",
        "      scores.append(score)\n",
        "    avg_scores.append(sum(scores) / len(scores))\n",
        "  return sum(avg_scores) / len(avg_scores)\n",
        "# all_outputs = [[\"hi how are you doing today\", \"hello how is your mother\", \"you looking good today ;)\"]]"
      ],
      "metadata": {
        "id": "f47MqfXLjIx2"
      },
      "id": "f47MqfXLjIx2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_volatility(num_samples, num_copies, max_temp):\n",
        "  dialogpt_bleu_scores = []\n",
        "  dialogpt_bleu_avgs = []\n",
        "\n",
        "  dialogpt_ss_scores = []\n",
        "  dialogpt_ss_avgs = []\n",
        "\n",
        "  blenderbot_bleu_scores = []\n",
        "  blenderbot_bleu_avgs = []\n",
        "\n",
        "  blenderbot_ss_scores = []\n",
        "  blenderbot_ss_avgs = []\n",
        "  # for temp in range(2, int(7 * max_temp)):\n",
        "  #   temp = temp / 10\n",
        "  #   dialogpt_outputs = get_dialogpt_outputs(num_samples, num_copies, temp)\n",
        "\n",
        "  #   dialogpt_bleu_score, dialogpt_bleu_avg = bleu(dialogpt_outputs)\n",
        "  #   dialogpt_bleu_scores.append(dialogpt_bleu_score)\n",
        "  #   dialogpt_bleu_avgs.append(dialogpt_bleu_avg)\n",
        "\n",
        "  #   dialogpt_ss_score, dialogpt_avg = semantic_similarity(dialogpt_outputs)\n",
        "  #   dialogpt_ss_scores.append(dialogpt_ss_score)\n",
        "  #   dialogpt_ss_avgs.append(dialogpt_avg)\n",
        "\n",
        "  # list_to_json(dialogpt_bleu_scores, \"dialogpt_bleu_scores.json\")\n",
        "  # list_to_json(dialogpt_bleu_avgs, \"dialogpt_bleu_avgs.json\")\n",
        "\n",
        "  # list_to_json(dialogpt_ss_scores, \"dialogpt_ss_scores.json\")\n",
        "  # list_to_json(dialogpt_ss_avgs, \"dialogpt_ss_avgs.json\")\n",
        "\n",
        "  for temp in range(8, int(10 * max_temp)):\n",
        "\n",
        "    temp = temp / 10\n",
        "    blenderbot_outputs = get_blenderbot_outputs(num_samples, num_copies * 2, temp)\n",
        "\n",
        "    blenderbot_bleu_score, blenderbot_bleu_avg = bleu(blenderbot_outputs)\n",
        "    blenderbot_bleu_scores.append(blenderbot_bleu_score)\n",
        "    blenderbot_bleu_avgs.append(blenderbot_bleu_avg)\n",
        "\n",
        "    blenderbot_ss_score, blenderbot_ss_avg = semantic_similarity(blenderbot_outputs)\n",
        "    blenderbot_ss_scores.append(blenderbot_ss_score)\n",
        "    blenderbot_ss_avgs.append(blenderbot_ss_avg)\n",
        "\n",
        "  list_to_json(blenderbot_bleu_scores, \"blenderbot_bleu_scores.json\")\n",
        "  list_to_json(blenderbot_bleu_avgs, \"blenderbot_bleu_avgs.json\")\n",
        "\n",
        "  list_to_json(blenderbot_ss_scores, \"blenderbot_ss_scores.json\")\n",
        "  list_to_json(blenderbot_ss_avgs, \"blenderbot_ss_avgs.json\")\n",
        "\n",
        "model_volatility(60, 10, 1.7)"
      ],
      "metadata": {
        "id": "Mz5ynhNL7t7J"
      },
      "id": "Mz5ynhNL7t7J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pairwise_model_volatility(num_samples, num_copies, max_temp):\n",
        "  dialogpt_bleu_avgs = []\n",
        "  dialogpt_ss_avgs = []\n",
        "\n",
        "  # for temp in range(3, max_temp):\n",
        "  #   dialogpt_temp_bleu_avgs = []\n",
        "  #   dialogpt_temp_ss_avgs = []\n",
        "\n",
        "  #   temp = temp / 10\n",
        "  #   dialogpt_outputs = get_dialogpt_outputs(num_samples, num_copies * 2, temp)\n",
        "  #   for i in dialogpt_outputs:\n",
        "  #     first = i[0:num_copies]\n",
        "  #     second = i[num_copies:]\n",
        "  #     dialogpt_bleu_avg = pairwise_bleu(first, second)\n",
        "  #     dialogpt_temp_bleu_avgs.append(dialogpt_bleu_avg)\n",
        "\n",
        "  #     dialogpt_ss_avg = pairwise_semantic_similarity(first, second)\n",
        "  #     dialogpt_temp_ss_avgs.append(dialogpt_ss_avg)\n",
        "\n",
        "  #   dialogpt_bleu_avgs.append(dialogpt_temp_bleu_avgs)\n",
        "  #   dialogpt_ss_avgs.append(dialogpt_temp_ss_avgs)\n",
        "\n",
        "  # list_to_json(dialogpt_bleu_avgs, \"data/dialogpt_bleu_pairwise_avgs.json\")\n",
        "  # list_to_json(dialogpt_ss_avgs, \"data/dialogpt_ss_pairwise_avgs.json\")\n",
        "\n",
        "  blenderbot_bleu_avgs = []\n",
        "  blenderbot_ss_avgs = []\n",
        "\n",
        "  for temp in range(11, max_temp):\n",
        "    blenderbot_temp_bleu_avgs = []\n",
        "    blenderbot_temp_ss_avgs = []\n",
        "\n",
        "    temp = temp / 10\n",
        "    blenderbot_outputs = get_blenderbot_outputs(num_samples, num_copies * 2, temp)\n",
        "\n",
        "    for output in blenderbot_outputs:\n",
        "      first = output[0:num_copies]\n",
        "      second = output[num_copies:]\n",
        "      blenderbot_bleu_avg = pairwise_bleu(first, second)\n",
        "      blenderbot_temp_bleu_avgs.append(blenderbot_bleu_avg)\n",
        "\n",
        "      blenderbot_ss_avg = pairwise_semantic_similarity(first, second)\n",
        "      blenderbot_temp_ss_avgs.append(blenderbot_ss_avg)\n",
        "\n",
        "    blenderbot_bleu_avgs.append(blenderbot_temp_bleu_avgs)\n",
        "    blenderbot_ss_avgs.append(blenderbot_temp_ss_avgs)\n",
        "\n",
        "  list_to_json(blenderbot_bleu_avgs, \"data/blenderbot_bleu_pairwise_avgs.json\")\n",
        "  list_to_json(blenderbot_ss_avgs, \"data/blenderbot_ss_pairwise_avgs.json\")\n",
        "\n",
        "pairwise_model_volatility(100, 4, 15)"
      ],
      "metadata": {
        "id": "jxdksxb-BjDL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "outputId": "efe8916c-9b2f-4328-b74d-2f04b5d6422c"
      },
      "id": "jxdksxb-BjDL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 14%|█▍        | 7/50 [01:32<09:30, 13.26s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-88f736c9226d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0mlist_to_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblenderbot_ss_avgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data/blenderbot_ss_pairwise_avgs.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mpairwise_model_volatility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-88f736c9226d>\u001b[0m in \u001b[0;36mpairwise_model_volatility\u001b[0;34m(num_samples, num_copies, max_temp)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mblenderbot_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_blenderbot_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_copies\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblenderbot_outputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-f2e5801c38e9>\u001b[0m in \u001b[0;36mget_blenderbot_outputs\u001b[0;34m(num_samples, num_copies, temp)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtemperature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mnum_return_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_copies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         attention_mask = attention_mask)\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Decode outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1299\u001b[0m                 \u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m                 \u001b[0msynced_gpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynced_gpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1302\u001b[0m             )\n\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mbeam_sample\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2352\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2353\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2354\u001b[0;31m                 \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2355\u001b[0m             )\n\u001b[1;32m   2356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/blenderbot/modeling_blenderbot.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1311\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1313\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1314\u001b[0m         )\n\u001b[1;32m   1315\u001b[0m         \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_logits_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/blenderbot/modeling_blenderbot.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1177\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1180\u001b[0m         )\n\u001b[1;32m   1181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/blenderbot/modeling_blenderbot.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m                     \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m                 )\n\u001b[1;32m   1026\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/blenderbot/modeling_blenderbot.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0mlayer_head_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_head_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m         )\n\u001b[1;32m    414\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/blenderbot/modeling_blenderbot.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_states\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;31m# self_attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 106.00 MiB (GPU 0; 15.90 GiB total capacity; 13.89 GiB already allocated; 35.75 MiB free; 14.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def data_viz(scores, start_temp):\n",
        "  y = scores\n",
        "  x = (np.arange(len(y)) / 10) + start_temp\n",
        "  plt.plot(x, y)\n",
        "  plt.ylabel(\"# of subreddits\")\n",
        "  plt.xlabel(\"subreddit size\")\n",
        "  plt.show()\n",
        "  "
      ],
      "metadata": {
        "id": "44j31vHGMhpO"
      },
      "id": "44j31vHGMhpO",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Double_Pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}